# generated by datamodel-codegen:
#   filename:  openapi.json
#   timestamp: 2024-01-11T21:14:55+00:00

from __future__ import annotations

from typing import Any, Dict, List

from typing_extensions import Literal, NotRequired, TypedDict


class ErrorOut(TypedDict):
    type: Literal['api_error', 'invalid_request_error']
    """
    The type of error returned.
    """
    message: str
    """
    A message providing more details about the error.
    """


class StableDiffusionInputImage(TypedDict):
    image_url: str
    """
    Input image URL to modify with a text prompt.
    """
    mask_image_url: NotRequired[str]
    """
    Mask of the input image to selectively modify.
    """
    prompt_strength: NotRequired[float]
    """
    Controls the influence of the input prompt on the generated output.
    """


class StableDiffusionIn(TypedDict):
    prompt: str
    """
    Input prompt.
    """
    negative_prompt: NotRequired[str]
    """
    Negative input prompt.
    """
    image: NotRequired[StableDiffusionInputImage]
    """
    Image parameters to modify an existing image (image-to-image).
    """
    store: NotRequired[bool]
    """
    Return a hosted image URL instead of base64 encoded image data.
    """
    width: NotRequired[int]
    """
    Width of output image, in pixels.
    """
    height: NotRequired[int]
    """
    Height of output image, in pixels.
    """
    steps: NotRequired[int]
    """
    Number of diffusion steps to run.
    """
    seed: NotRequired[int]
    """
    Random noise seed. Default is a random seed.
    """
    guidance_scale: NotRequired[int]
    """
    Controls the influence of the input prompt on the generated output.
    """
    num_images: NotRequired[int]
    """
    Number of images to generate.
    """


class StableDiffusionImage(TypedDict):
    uri: str
    """
    Base 64-encoded JPEG image bytes, or a hosted image url if `store` is enabled.
    """
    seed: int
    """
    The random noise seed used for generation.
    """


class StableDiffusionOut(TypedDict):
    items: List[StableDiffusionImage]


class StableDiffusionResponse(TypedDict):
    data: NotRequired[StableDiffusionOut]
    error: NotRequired[ErrorOut]


class MistralPrompt(TypedDict):
    prompt: str
    """
    Prompt to generate completions for.
    """


class MistralIn(TypedDict):
    prompts: List[MistralPrompt]
    """
    Input prompts.
    """
    temperature: NotRequired[float]
    """
    Sampling temperature to use. Higher values make the output more random; lower values make the output more deterministic.
    """
    max_tokens: NotRequired[int]
    """
    Maximum number of tokens to generate.
    """
    num_completions: NotRequired[int]
    """
    Number of completions to generate for each prompt.
    """


class MistralResult(TypedDict):
    completions: List[str]
    """
    Generated completion choices.
    """


class MistralOut(TypedDict):
    items: List[MistralResult]


class MistralResponse(TypedDict):
    data: NotRequired[MistralOut]
    error: NotRequired[ErrorOut]


class WhisperIn(TypedDict):
    audio_url: str
    """
    Input audio URL.
    """
    prompt: NotRequired[str]
    """
    Prompt to guide model on contents of input audio and desired output.
    """
    language: NotRequired[str]
    """
    Language of input audio in ISO-639-1 format.
    """
    align: NotRequired[bool]
    """
    Align transcription to produce more accurate sentence-level timestamps and word-level timestamps. An array of word segments will be included in each sentence segment.
    """
    diarize: NotRequired[bool]
    """
    Identify speakers for each segment. Speaker IDs will be added to each output segment.
    """


class WhisperWord(TypedDict):
    word: str
    """
    Text of word.
    """
    start: float
    """
    Start time of word, in seconds.
    """
    end: float
    """
    End time of word, in seconds.
    """
    speaker: NotRequired[str]
    """
    ID of speaker, if `diarize` is enabled.
    """


class WhisperSegment(TypedDict):
    text: str
    """
    Text of segment.
    """
    start: float
    """
    Start time of segment, in seconds.
    """
    end: float
    """
    End time of segment, in seconds.
    """
    speaker: NotRequired[str]
    """
    ID of speaker, if `diarize` is enabled.
    """
    words: NotRequired[List[WhisperWord]]
    """
    Aligned words, if `align` is enabled.
    """


class WhisperOut(TypedDict):
    items: List[WhisperSegment]


class WhisperResponse(TypedDict):
    data: NotRequired[WhisperOut]
    error: NotRequired[ErrorOut]


class JinaDoc(TypedDict):
    text: str
    """
    Text to embed.
    """
    id: NotRequired[str]
    """
    Document id. Required when storing embedding.
    """
    metadata: NotRequired[Dict[str, Any]]
    """
    Additional metadata to store in embedding document.
    """
    embed_metadata_keys: NotRequired[List[str]]
    """
    Contents of `metadata` included to generate embedding.
    """


class JinaIn(TypedDict):
    docs: List[JinaDoc]
    """
    Documents to embed.
    """
    store: NotRequired[str]
    """
    Vector store ID in which embedding will be stored.
    """


class JinaEmbedding(TypedDict):
    vector: List[float]
    """
    Embedding vector.
    """
    id: NotRequired[str]
    """
    Document id.
    """
    metadata: NotRequired[Dict[str, Any]]
    """
    Additional metadata stored in embedding document.
    """


class JinaOut(TypedDict):
    items: List[JinaEmbedding]


class JinaResponse(TypedDict):
    data: NotRequired[JinaOut]
    error: NotRequired[ErrorOut]


class CLIPDoc(TypedDict):
    text: NotRequired[str]
    """
    Text to embed.
    """
    image_url: NotRequired[str]
    """
    URL of image to embed.
    """
    id: NotRequired[str]
    """
    Document id. Required when storing embedding.
    """
    metadata: NotRequired[Dict[str, Any]]
    """
    Additional metadata to store in embedding document.
    """
    embed_metadata_keys: NotRequired[List[str]]
    """
    Contents of `metadata` included to generate embedding.
    """


class CLIPIn(TypedDict):
    docs: List[CLIPDoc]
    """
    Documents to embed.
    """
    store: NotRequired[str]
    """
    Vector store ID in which embedding will be stored.
    """


class CLIPEmbedding(TypedDict):
    vector: List[float]
    """
    Embedding vector.
    """
    id: NotRequired[str]
    """
    Document id.
    """
    metadata: NotRequired[Dict[str, Any]]
    """
    Additional metadata stored in embedding document.
    """


class CLIPOut(TypedDict):
    items: List[CLIPEmbedding]


class CLIPResponse(TypedDict):
    data: NotRequired[CLIPOut]
    error: NotRequired[ErrorOut]
