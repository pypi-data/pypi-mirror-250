<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>abstochkin.agentstatedata API documentation</title>
<meta name="description" content="Class for storing the state of all agents of a certain species during an
AbStochKin simulation." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}#lunr-search{width:100%;font-size:1em;padding:6px 9px 5px 9px;border:1px solid silver}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>abstochkin.agentstatedata</code></h1>
</header>
<section id="section-intro">
<p>Class for storing the state of all agents of a certain species during an
AbStochKin simulation.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
Class for storing the state of all agents of a certain species during an
AbStochKin simulation.
&#34;&#34;&#34;

#  Copyright (c) 2023, Alex Plakantonakis.
#
#  This program is free software: you can redistribute it and/or modify
#  it under the terms of the GNU General Public License as published by
#  the Free Software Foundation, either version 3 of the License, or
#  (at your option) any later version.
#
#  This program is distributed in the hope that it will be useful,
#  but WITHOUT ANY WARRANTY; without even the implied warranty of
#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#  GNU General Public License for more details.
#
#  You should have received a copy of the GNU General Public License
#  along with this program.  If not, see &lt;http://www.gnu.org/licenses/&gt;.

from copy import deepcopy
from dataclasses import dataclass, field

import numpy as np


@dataclass
class AgentStateData:
    &#34;&#34;&#34;
    Class for storing the state of all agents of a certain species during an
    AbStochKin simulation.

    Attributes
    ----------
    p_init : int
        The initial population size of the species whose data is
        represented in an `AgentStateData` object.
    max_agents : int
        The maximum number of agents for the species whose data
        is represented in an `AgentStateData` object.
    reps : int
        The number of times the AbStochKin algorithm will repeat a simulation.
        This will be the length of the `asv` list.
    fill_state: int

    asv_ini : numpy.ndarray
        Agent-State Vector (asv) is a species-specific 2-row vector to monitor
        agent state according to Markov&#39;s property. This array is the initial
        asv, i.e., at `t=0`. The array shape is `(2, max_agents)`
    asv : list of numpy.ndarray
        A list of length `reps` with copies of `asv_ini`. Each simulation run
        uses its corresponding entry in `asv` to monitor the state of all
        agents.
    &#34;&#34;&#34;

    p_init: int  # initial population size
    max_agents: int  # maximum number of agents represented in asv
    reps: int  # number of times simulation is repeated
    fill_state: int

    asv_ini: np.ndarray = field(init=False, default_factory=lambda: np.array([]))
    asv: list[np.ndarray] = field(init=False, default_factory=lambda: list(np.array([])))

    def __post_init__(self):
        # Set up initial (t=0) agent-state vector (asv):
        self.asv_ini = np.concatenate(
            (np.ones(shape=(2, self.p_init), dtype=np.int8),
             np.full(shape=(2, self.max_agents - self.p_init),
                     fill_value=self.fill_state,
                     dtype=np.int8)),
            axis=1
        )

        # Set up separate copy of the initial `asv` for each repetition of the
        # algorithm to facilitate parallelization of ensemble runs.
        self.asv = [deepcopy(self.asv_ini) for _ in range(self.reps)]

    def apply_markov_property(self, r: int):
        &#34;&#34;&#34;
        The future state of the system depends only on its current state.
        This method is called at the end of each time step in an AbStochKin
        simulation. Therefore, the new agent-state vector becomes the
        current state.
        &#34;&#34;&#34;
        self.asv[r][0, :] = self.asv[r][1, :]

    def cleanup_asv(self):
        &#34;&#34;&#34; Empty the contents of the array `asv`. &#34;&#34;&#34;
        self.asv = list(np.array([]))

    def get_vals_o1(self,
                    r: int,
                    stream: np.random.Generator,
                    p_vals: np.ndarray,
                    state: int = 1):
        &#34;&#34;&#34;
        Get random values in [0,1) at a given time step for agents of a given
        state. Agents of other states have a value of zero.

        Get probability values at a given time step for agents of a given state.
        Agents of other states have a transition probability of zero.

        Notes
        -----
        Note that only elements of the `asv` that have the same state in the
        previous and current time steps are considered. This is to ensure that
        agents that have already transitioned to a different state in the
        current time step are not reconsidered for a possible transition.
        &#34;&#34;&#34;
        nonzero_elems = np.all(self.asv[r] == state, axis=0)
        final_rand_nums = stream.random(self.max_agents) * nonzero_elems
        final_p_vals = p_vals * nonzero_elems

        return final_rand_nums, final_p_vals

    def get_vals_o2(self,
                    other,
                    r: int,
                    stream: np.random.Generator,
                    p_vals: np.ndarray,
                    state: int = 1):
        &#34;&#34;&#34;
        Get random values in [0,1) at a given time step for interactions between
        agents of a given state. Agents of other states have a value of zero.

        Get probability values at a given time step for interactions between
        agents of a given state. Interactions of agents in other states
        have a transition probability of zero.

        Notes
        -----
        Note that only elements of the `asv` that have the same state in the
        previous and current time steps are considered. This is to ensure that
        agents that have already transitioned to a different state in the
        current time step are not reconsidered for a possible transition.
        &#34;&#34;&#34;
        nonzero_rows = np.all(self.asv[r] == state, axis=0).reshape(-1, 1)
        nonzero_cols = np.all(other.asv[r] == state, axis=0).reshape(1, -1)

        rand_nums = stream.random(size=(self.max_agents, other.max_agents))

        final_rand_nums = rand_nums * nonzero_rows * nonzero_cols
        final_p_vals = p_vals * nonzero_rows * nonzero_cols

        return final_rand_nums, final_p_vals

    def __str__(self):
        return f&#34;Agent-State Vector with \n&#34; \
               f&#34;Initial population size: {self.p_init}\n&#34; \
               f&#34;Maximum number of agents: {self.max_agents}\n&#34; \
               f&#34;Repeat simulation {self.reps} times\n&#34; \
               f&#34;Fill state: {self.fill_state}&#34;</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="abstochkin.agentstatedata.AgentStateData"><code class="flex name class">
<span>class <span class="ident">AgentStateData</span></span>
<span>(</span><span>p_init: int, max_agents: int, reps: int, fill_state: int)</span>
</code></dt>
<dd>
<div class="desc"><p>Class for storing the state of all agents of a certain species during an
AbStochKin simulation.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>p_init</code></strong> :&ensp;<code>int</code></dt>
<dd>The initial population size of the species whose data is
represented in an <code><a title="abstochkin.agentstatedata.AgentStateData" href="#abstochkin.agentstatedata.AgentStateData">AgentStateData</a></code> object.</dd>
<dt><strong><code>max_agents</code></strong> :&ensp;<code>int</code></dt>
<dd>The maximum number of agents for the species whose data
is represented in an <code><a title="abstochkin.agentstatedata.AgentStateData" href="#abstochkin.agentstatedata.AgentStateData">AgentStateData</a></code> object.</dd>
<dt><strong><code>reps</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of times the AbStochKin algorithm will repeat a simulation.
This will be the length of the <code>asv</code> list.</dd>
<dt><strong><code>fill_state</code></strong> :&ensp;<code>int</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>asv_ini</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>Agent-State Vector (asv) is a species-specific 2-row vector to monitor
agent state according to Markov's property. This array is the initial
asv, i.e., at <code>t=0</code>. The array shape is <code>(2, max_agents)</code></dd>
<dt><strong><code>asv</code></strong> :&ensp;<code>list</code> of <code>numpy.ndarray</code></dt>
<dd>A list of length <code>reps</code> with copies of <code>asv_ini</code>. Each simulation run
uses its corresponding entry in <code>asv</code> to monitor the state of all
agents.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@dataclass
class AgentStateData:
    &#34;&#34;&#34;
    Class for storing the state of all agents of a certain species during an
    AbStochKin simulation.

    Attributes
    ----------
    p_init : int
        The initial population size of the species whose data is
        represented in an `AgentStateData` object.
    max_agents : int
        The maximum number of agents for the species whose data
        is represented in an `AgentStateData` object.
    reps : int
        The number of times the AbStochKin algorithm will repeat a simulation.
        This will be the length of the `asv` list.
    fill_state: int

    asv_ini : numpy.ndarray
        Agent-State Vector (asv) is a species-specific 2-row vector to monitor
        agent state according to Markov&#39;s property. This array is the initial
        asv, i.e., at `t=0`. The array shape is `(2, max_agents)`
    asv : list of numpy.ndarray
        A list of length `reps` with copies of `asv_ini`. Each simulation run
        uses its corresponding entry in `asv` to monitor the state of all
        agents.
    &#34;&#34;&#34;

    p_init: int  # initial population size
    max_agents: int  # maximum number of agents represented in asv
    reps: int  # number of times simulation is repeated
    fill_state: int

    asv_ini: np.ndarray = field(init=False, default_factory=lambda: np.array([]))
    asv: list[np.ndarray] = field(init=False, default_factory=lambda: list(np.array([])))

    def __post_init__(self):
        # Set up initial (t=0) agent-state vector (asv):
        self.asv_ini = np.concatenate(
            (np.ones(shape=(2, self.p_init), dtype=np.int8),
             np.full(shape=(2, self.max_agents - self.p_init),
                     fill_value=self.fill_state,
                     dtype=np.int8)),
            axis=1
        )

        # Set up separate copy of the initial `asv` for each repetition of the
        # algorithm to facilitate parallelization of ensemble runs.
        self.asv = [deepcopy(self.asv_ini) for _ in range(self.reps)]

    def apply_markov_property(self, r: int):
        &#34;&#34;&#34;
        The future state of the system depends only on its current state.
        This method is called at the end of each time step in an AbStochKin
        simulation. Therefore, the new agent-state vector becomes the
        current state.
        &#34;&#34;&#34;
        self.asv[r][0, :] = self.asv[r][1, :]

    def cleanup_asv(self):
        &#34;&#34;&#34; Empty the contents of the array `asv`. &#34;&#34;&#34;
        self.asv = list(np.array([]))

    def get_vals_o1(self,
                    r: int,
                    stream: np.random.Generator,
                    p_vals: np.ndarray,
                    state: int = 1):
        &#34;&#34;&#34;
        Get random values in [0,1) at a given time step for agents of a given
        state. Agents of other states have a value of zero.

        Get probability values at a given time step for agents of a given state.
        Agents of other states have a transition probability of zero.

        Notes
        -----
        Note that only elements of the `asv` that have the same state in the
        previous and current time steps are considered. This is to ensure that
        agents that have already transitioned to a different state in the
        current time step are not reconsidered for a possible transition.
        &#34;&#34;&#34;
        nonzero_elems = np.all(self.asv[r] == state, axis=0)
        final_rand_nums = stream.random(self.max_agents) * nonzero_elems
        final_p_vals = p_vals * nonzero_elems

        return final_rand_nums, final_p_vals

    def get_vals_o2(self,
                    other,
                    r: int,
                    stream: np.random.Generator,
                    p_vals: np.ndarray,
                    state: int = 1):
        &#34;&#34;&#34;
        Get random values in [0,1) at a given time step for interactions between
        agents of a given state. Agents of other states have a value of zero.

        Get probability values at a given time step for interactions between
        agents of a given state. Interactions of agents in other states
        have a transition probability of zero.

        Notes
        -----
        Note that only elements of the `asv` that have the same state in the
        previous and current time steps are considered. This is to ensure that
        agents that have already transitioned to a different state in the
        current time step are not reconsidered for a possible transition.
        &#34;&#34;&#34;
        nonzero_rows = np.all(self.asv[r] == state, axis=0).reshape(-1, 1)
        nonzero_cols = np.all(other.asv[r] == state, axis=0).reshape(1, -1)

        rand_nums = stream.random(size=(self.max_agents, other.max_agents))

        final_rand_nums = rand_nums * nonzero_rows * nonzero_cols
        final_p_vals = p_vals * nonzero_rows * nonzero_cols

        return final_rand_nums, final_p_vals

    def __str__(self):
        return f&#34;Agent-State Vector with \n&#34; \
               f&#34;Initial population size: {self.p_init}\n&#34; \
               f&#34;Maximum number of agents: {self.max_agents}\n&#34; \
               f&#34;Repeat simulation {self.reps} times\n&#34; \
               f&#34;Fill state: {self.fill_state}&#34;</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="abstochkin.agentstatedata.AgentStateData.asv"><code class="name">var <span class="ident">asv</span> : list[numpy.ndarray]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="abstochkin.agentstatedata.AgentStateData.asv_ini"><code class="name">var <span class="ident">asv_ini</span> : numpy.ndarray</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="abstochkin.agentstatedata.AgentStateData.fill_state"><code class="name">var <span class="ident">fill_state</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="abstochkin.agentstatedata.AgentStateData.max_agents"><code class="name">var <span class="ident">max_agents</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="abstochkin.agentstatedata.AgentStateData.p_init"><code class="name">var <span class="ident">p_init</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="abstochkin.agentstatedata.AgentStateData.reps"><code class="name">var <span class="ident">reps</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="abstochkin.agentstatedata.AgentStateData.apply_markov_property"><code class="name flex">
<span>def <span class="ident">apply_markov_property</span></span>(<span>self, r: int)</span>
</code></dt>
<dd>
<div class="desc"><p>The future state of the system depends only on its current state.
This method is called at the end of each time step in an AbStochKin
simulation. Therefore, the new agent-state vector becomes the
current state.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def apply_markov_property(self, r: int):
    &#34;&#34;&#34;
    The future state of the system depends only on its current state.
    This method is called at the end of each time step in an AbStochKin
    simulation. Therefore, the new agent-state vector becomes the
    current state.
    &#34;&#34;&#34;
    self.asv[r][0, :] = self.asv[r][1, :]</code></pre>
</details>
</dd>
<dt id="abstochkin.agentstatedata.AgentStateData.cleanup_asv"><code class="name flex">
<span>def <span class="ident">cleanup_asv</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Empty the contents of the array <code>asv</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cleanup_asv(self):
    &#34;&#34;&#34; Empty the contents of the array `asv`. &#34;&#34;&#34;
    self.asv = list(np.array([]))</code></pre>
</details>
</dd>
<dt id="abstochkin.agentstatedata.AgentStateData.get_vals_o1"><code class="name flex">
<span>def <span class="ident">get_vals_o1</span></span>(<span>self, r: int, stream: numpy.random._generator.Generator, p_vals: numpy.ndarray, state: int = 1)</span>
</code></dt>
<dd>
<div class="desc"><p>Get random values in [0,1) at a given time step for agents of a given
state. Agents of other states have a value of zero.</p>
<p>Get probability values at a given time step for agents of a given state.
Agents of other states have a transition probability of zero.</p>
<h2 id="notes">Notes</h2>
<p>Note that only elements of the <code>asv</code> that have the same state in the
previous and current time steps are considered. This is to ensure that
agents that have already transitioned to a different state in the
current time step are not reconsidered for a possible transition.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_vals_o1(self,
                r: int,
                stream: np.random.Generator,
                p_vals: np.ndarray,
                state: int = 1):
    &#34;&#34;&#34;
    Get random values in [0,1) at a given time step for agents of a given
    state. Agents of other states have a value of zero.

    Get probability values at a given time step for agents of a given state.
    Agents of other states have a transition probability of zero.

    Notes
    -----
    Note that only elements of the `asv` that have the same state in the
    previous and current time steps are considered. This is to ensure that
    agents that have already transitioned to a different state in the
    current time step are not reconsidered for a possible transition.
    &#34;&#34;&#34;
    nonzero_elems = np.all(self.asv[r] == state, axis=0)
    final_rand_nums = stream.random(self.max_agents) * nonzero_elems
    final_p_vals = p_vals * nonzero_elems

    return final_rand_nums, final_p_vals</code></pre>
</details>
</dd>
<dt id="abstochkin.agentstatedata.AgentStateData.get_vals_o2"><code class="name flex">
<span>def <span class="ident">get_vals_o2</span></span>(<span>self, other, r: int, stream: numpy.random._generator.Generator, p_vals: numpy.ndarray, state: int = 1)</span>
</code></dt>
<dd>
<div class="desc"><p>Get random values in [0,1) at a given time step for interactions between
agents of a given state. Agents of other states have a value of zero.</p>
<p>Get probability values at a given time step for interactions between
agents of a given state. Interactions of agents in other states
have a transition probability of zero.</p>
<h2 id="notes">Notes</h2>
<p>Note that only elements of the <code>asv</code> that have the same state in the
previous and current time steps are considered. This is to ensure that
agents that have already transitioned to a different state in the
current time step are not reconsidered for a possible transition.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_vals_o2(self,
                other,
                r: int,
                stream: np.random.Generator,
                p_vals: np.ndarray,
                state: int = 1):
    &#34;&#34;&#34;
    Get random values in [0,1) at a given time step for interactions between
    agents of a given state. Agents of other states have a value of zero.

    Get probability values at a given time step for interactions between
    agents of a given state. Interactions of agents in other states
    have a transition probability of zero.

    Notes
    -----
    Note that only elements of the `asv` that have the same state in the
    previous and current time steps are considered. This is to ensure that
    agents that have already transitioned to a different state in the
    current time step are not reconsidered for a possible transition.
    &#34;&#34;&#34;
    nonzero_rows = np.all(self.asv[r] == state, axis=0).reshape(-1, 1)
    nonzero_cols = np.all(other.asv[r] == state, axis=0).reshape(1, -1)

    rand_nums = stream.random(size=(self.max_agents, other.max_agents))

    final_rand_nums = rand_nums * nonzero_rows * nonzero_cols
    final_p_vals = p_vals * nonzero_rows * nonzero_cols

    return final_rand_nums, final_p_vals</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<form>
<input id="lunr-search" name="q" placeholder="🔎 Search ..." aria-label="Search"
disabled minlength="2">
</form>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/tingle/0.15.3/tingle.min.css" integrity="sha512-j1u8eUJ4f23xPPxwOrLUPQaCD2dwzNqqmDDcWS4deWsMv2ohLqmXXuP3hU7g8TyzbMSakP/mMqoNBYWj8AEIFg==" crossorigin>
<script src="https://cdnjs.cloudflare.com/ajax/libs/tingle/0.15.3/tingle.min.js" integrity="sha512-plGUER9JkeEWPPqQBE4sdLqBoQug5Ap+BCGMc7bJ8BXkm+VVj6QzkpBz5Yv2yPkkq+cqg9IpkBaGCas6uDbW8g==" crossorigin></script>
<style>
.modal-dialog iframe {
width: 100vw;
height: calc(100vh - 80px);
}
@media screen and (min-width: 700px) {
.modal-dialog iframe {
width: 70vw;
height: 80vh;
}
}
.modal-dialog .tingle-modal-box {width: auto;}
.modal-dialog .tingle-modal-box__content {padding: 0;}
</style>
<script>
const input = document.getElementById('lunr-search');
input.disabled = false;
input.form.addEventListener('submit', (ev) => {
ev.preventDefault();
const url = new URL(window.location);
url.searchParams.set('q', input.value);
history.replaceState({}, null, url.toString());
search(input.value);
});
const query = new URL(window.location).searchParams.get('q');
if (query)
search(query);
function search(query) {
const url = '../doc-search.html#' + encodeURIComponent(query);
new tingle.modal({
cssClass: ['modal-dialog'],
onClose: () => {
const url = new URL(window.location);
url.searchParams.delete('q');
history.replaceState({}, null, url.toString());
setTimeout(() => input.focus(), 100);
}
}).setContent('<iframe src="' + url + '"></iframe>').open();
}
</script>
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="abstochkin" href="index.html">abstochkin</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="abstochkin.agentstatedata.AgentStateData" href="#abstochkin.agentstatedata.AgentStateData">AgentStateData</a></code></h4>
<ul class="">
<li><code><a title="abstochkin.agentstatedata.AgentStateData.apply_markov_property" href="#abstochkin.agentstatedata.AgentStateData.apply_markov_property">apply_markov_property</a></code></li>
<li><code><a title="abstochkin.agentstatedata.AgentStateData.asv" href="#abstochkin.agentstatedata.AgentStateData.asv">asv</a></code></li>
<li><code><a title="abstochkin.agentstatedata.AgentStateData.asv_ini" href="#abstochkin.agentstatedata.AgentStateData.asv_ini">asv_ini</a></code></li>
<li><code><a title="abstochkin.agentstatedata.AgentStateData.cleanup_asv" href="#abstochkin.agentstatedata.AgentStateData.cleanup_asv">cleanup_asv</a></code></li>
<li><code><a title="abstochkin.agentstatedata.AgentStateData.fill_state" href="#abstochkin.agentstatedata.AgentStateData.fill_state">fill_state</a></code></li>
<li><code><a title="abstochkin.agentstatedata.AgentStateData.get_vals_o1" href="#abstochkin.agentstatedata.AgentStateData.get_vals_o1">get_vals_o1</a></code></li>
<li><code><a title="abstochkin.agentstatedata.AgentStateData.get_vals_o2" href="#abstochkin.agentstatedata.AgentStateData.get_vals_o2">get_vals_o2</a></code></li>
<li><code><a title="abstochkin.agentstatedata.AgentStateData.max_agents" href="#abstochkin.agentstatedata.AgentStateData.max_agents">max_agents</a></code></li>
<li><code><a title="abstochkin.agentstatedata.AgentStateData.p_init" href="#abstochkin.agentstatedata.AgentStateData.p_init">p_init</a></code></li>
<li><code><a title="abstochkin.agentstatedata.AgentStateData.reps" href="#abstochkin.agentstatedata.AgentStateData.reps">reps</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
© Copyright 2024, Alex Plakantonakis.
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>