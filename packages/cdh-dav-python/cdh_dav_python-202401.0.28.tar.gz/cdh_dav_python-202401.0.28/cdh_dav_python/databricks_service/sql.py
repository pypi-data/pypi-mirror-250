import traceback  # don't remove required for error handling
import json
import base64
import requests
import re
import os
import sys

from html.parser import HTMLParser  # web scraping html
from string import Formatter
from importlib import util  # library management

OS_NAME = os.name
sys.path.append("../..")

if OS_NAME.lower() == "nt":
    print("environment_logging: windows")
    sys.path.append(os.path.dirname(os.path.abspath(__file__ + "\\..")))
    sys.path.append(os.path.dirname(os.path.abspath(__file__ + "\\..\\..")))
    sys.path.append(os.path.dirname(os.path.abspath(__file__ + "\\..\\..\\..")))
else:
    print("environment_logging: non windows")
    sys.path.append(os.path.dirname(os.path.abspath(__file__ + "/..")))
    sys.path.append(os.path.dirname(os.path.abspath(__file__ + "/../..")))
    sys.path.append(os.path.dirname(os.path.abspath(__file__ + "/../../..")))


import cdh_dav_python.cdc_admin_service.environment_tracing as cdc_env_tracing
import cdh_dav_python.cdc_admin_service.environment_logging as cdc_env_logging

# Get the currently running file name
NAMESPACE_NAME = os.path.basename(os.path.dirname(__file__))
# Get the parent folder name of the running file
SERVICE_NAME = os.path.basename(__file__)


class DatabricksSQL:
    """
    A class that provides methods for interacting with Databricks SQL queries.
    """

    @staticmethod
    def save_pipeline_sql(
        access_token,
        repository_path,
        yyyy_param,
        mm_param,
        dd_param,
        environment,
        databricks_instance_id,
        data_product_id_root,
        data_product_id,
        query_name,
        pipeline_name,
        execute_results_flag,
        arg_dictionary,
        transmission_period,
    ):
        """
        Saves a pipeline SQL query to Databricks.

        Parameters:
        - access_token (str): The access token for authentication.
        - repository_path (str): The path to the repository.
        - yyyy_param (str): The year parameter.
        - mm_param (str): The month parameter.
        - dd_param (str): The day parameter.
        - environment (str): The environment.
        - databricks_instance_id (str): The Databricks instance ID.
        - data_product_id_root (str): The root data product ID.
        - data_product_id (str): The data product ID.
        - query_name (str): The name of the query.
        - pipeline_name (str): The name of the pipeline.
        - execute_results_flag (str): The flag for executing results.
        - arg_dictionary (dict): A dictionary of argument values.
        - transmission_period (str): The transmission period.
        """

        logger_singleton = cdc_env_logging.LoggerSingleton.instance(
            NAMESPACE_NAME, SERVICE_NAME
        )

        logger = logger_singleton.get_logger()

        tracer_singleton = cdc_env_tracing.TracerSingleton.instance(
            NAMESPACE_NAME, SERVICE_NAME
        )
        tracer = tracer_singleton.get_tracer()

        with tracer.start_as_current_span("save_pipeline_sql"):
            try:
                base_path = "".join(
                    [
                        repository_path.rstrip("/"),
                        "/",
                        data_product_id_root,
                        "/",
                        data_product_id,
                        "/",
                    ]
                )
                base_path = base_path.replace("/Workspace", "")
                dir_name_python = "".join(
                    [base_path.rstrip("/"), "/autogenerated/python/"]
                )
                dir_name_sql = "".join([base_path.rstrip("/"), "/autogenerated/sql/"])
                dir_name_sql = dir_name_sql.replace("//", "/")
                message = f"dir_name_sql:{dir_name_sql}"
                logger.info(message)
                message = f"dir_name_python:{dir_name_python}"
                logger.info(message)
                bearer = "Bearer " + access_token
                headers = {"Authorization": bearer, "Content-Type": "application/json"}
                headers_redacted = str(headers).replace(bearer, "[bearer REDACTED]")
                api_version = "/api/2.0/preview/sql"
                text = [
                    "/queries?page_size=50&page=1&order=-executed_at&q=",
                    query_name,
                ]
                api_command = "".join(text)
                url = f"https://{databricks_instance_id}{api_version}{api_command}"
                logger.info(
                    f"- Attempting FETCH-SQL for query_name:{query_name} url:{str(url)} ----"
                )
                logger.info(f"headers:{headers_redacted}")

                try:
                    # Send the request
                    response = requests.get(url=url, headers=headers)

                    # Check if the request was successful
                    response.raise_for_status()

                    # Process the response
                    results = response.json()

                except requests.exceptions.HTTPError as http_err:
                    error_msg = "Error: %s", http_err
                    exc_info = sys.exc_info()
                    logger_singleton.error_with_exception(error_msg, exc_info)
                    raise

                except Exception as err:
                    error_msg = "Error: %s", err
                    exc_info = sys.exc_info()
                    logger_singleton.error_with_exception(error_msg, exc_info)
                    raise

                data = None
                data_sql = None

                try:
                    data = json.loads(response.text)
                    response_text = response.text
                    response_text_fetch = (
                        f"Received FETCH-SQL with length : {len(str(response_text))}"
                    )
                    response_text_fetch = (
                        response_text_fetch
                        + f" when posting to : {url} to fetch sql query: {query_name}"
                    )
                    logger.info("- response : success  -")
                    logger.info(f"{response_text_fetch}")
                except Exception as exception_check:
                    html_filter = HTMLFilter()
                    html_filter.feed(response.text)
                    response_text = html_filter.text
                    logger.error(f"- response : error - {str(exception_check)}")
                    logger.error(
                        f"Error converting response text:{response_text} to json"
                    )

                if data is None:
                    logger.info("Error loading sql query")
                else:
                    query_text = "# Check configuration of view in list - no query content was found"
                    response = "not set"

                    for i in data["results"]:
                        # print(i)
                        query_text_original = i["query"]
                        query_text = query_text_original.replace(
                            "{{", "TEMPORARY_OPEN_BRACKET"
                        ).replace("}}", "TEMPORARY_CLOSE_BRACKET")
                        query_text = query_text.replace("{", "{{").replace("}", "}}")
                        query_text = query_text.replace(
                            "TEMPORARY_OPEN_BRACKET", "{"
                        ).replace("TEMPORARY_CLOSE_BRACKET", "}")
                        query_text = query_text.lstrip()
                        query_text = query_text.rstrip()
                        query_text = query_text.replace('"', '\\"')

                        # remove -- comments
                        query_text = re.sub(
                            r"^--.*\n?", "", query_text, flags=re.MULTILINE
                        )

                        if query_text == "":
                            print(f"query name{query_name}:")
                            print(f"{query_text} not found in DataBricks SQL")
                        else:
                            if not query_text.endswith(";"):
                                query_text += ";"
                        ph = "TEMPORARY_OPEN_BRACKET"
                        variable_text = (
                            f'execute_results_flag = "{execute_results_flag}"'
                        )
                        query_parse = query_text_original.replace("{{", "{").replace(
                            "}}", "}"
                        )
                        # print(f"query_parse:{query_parse}")
                        param_list = [
                            fname
                            for _, fname, _, _ in Formatter().parse(query_parse)
                            if fname
                        ]
                        logger.info(f"param_list:{str(param_list)}")
                        dict_param_unique = dict()
                        for line in list(dict.fromkeys(param_list)):
                            line = line.replace('"', "").replace("'", "")
                            if line.strip() == "environment":
                                dict_param_unique[
                                    "'" + line.strip() + "'"
                                ] = environment
                            else:
                                dict_param_unique["'" + line.strip() + "'"] = (
                                    "'enter " + line.strip() + " value'"
                                )

                        dict_param_unique["yyyy"] = yyyy_param
                        dict_param_unique["mm"] = mm_param
                        dict_param_unique["dd"] = dd_param
                        dict_param_unique["transmission_period"] = transmission_period

                        sql_command_text = (
                            'sql_command_text = """'
                            + query_text
                            + '""".format(**dict_parameters)'
                        )
                        print_query_text = f"print(sql_command_text)"
                        print_df_results_text = """
        from pyspark.sql.functions import col
        dfResults = spark.sql(sql_command_text)
        #display(dfResults)
        listColumns=dfResults.columns
        #if ("sql_statement"  in listColumns):
        #    print(dfResults.first().sql_statement)
        if (dfResults.count() > 0):
            if ("sql_statement"  in listColumns):
                dfMerge = spark.sql(dfResults.first().sql_statement)
                display(dfMerge)
        """

                        new_param_code = ""
                        for line in dict_param_unique:
                            line = line.replace('"', "").replace("'", "")
                            # new_param_code = new_param_code + f"-- MAGIC {line} =  ''\n"
                            if line in arg_dictionary:
                                new_param_code = (
                                    new_param_code
                                    + f"dbutils.widgets.text('{line}', '{arg_dictionary[line]}')\n"
                                )
                            else:
                                print(f"{line} not in arg_dictionary")
                                new_param_code = (
                                    new_param_code
                                    + f"dbutils.widgets.text('{line}', 'default')\n"
                                )
                            new_param_code = (
                                new_param_code
                                + f"{line} = dbutils.widgets.get('{line}')\n"
                            )

                        dict_code = ""
                        for line in dict_param_unique:
                            line = line.replace('"', "").replace("'", "")
                            # new_param_code = new_param_code + f"-- MAGIC {line} =  ''\n"
                            if line in arg_dictionary:
                                line_strip = line.strip().replace('"', "")
                                dict_code = (
                                    dict_code
                                    + f"'{line_strip}':'{arg_dictionary[line]}',"
                                )
                            else:
                                print(f"{line} not in arg_dictionary")
                                line_strip = line.strip().replace('"', "")
                                dict_code = dict_code + f"'{line_strip}':'default',"

                        dict_code = dict_code + f"'environment':'{environment}',"
                        dict_parameters = (
                            "dict_parameters = {" + dict_code.rstrip(",") + "}\n"
                        )

                        new_param_code = new_param_code + dict_parameters

                        print(f"Set query parameters for {query_name}")
                        # print('query parameters from new' + new_param_code)
                        # print ('query parameters from ' + pipeline_name + '--------->' + new_param_code )
                        ###
                        # BE CAREFUL ABOUT SPACING - DON'T TRY GET FANCY WITH FORMAT BECAUSE
                        # MAY BREAK INDENTS WHICH ARE IMPORTANT TO PYTHON EXECUTION
                        ###

                        contexttext1 = f"""
        {new_param_code}
                        """
                        contexttext2 = " # COMMAND ----------\n"

                        contexttext3 = f"""{sql_command_text}\n"""

                        contexttext4 = " # COMMAND ----------\n"

                        contexttext5 = f"""{print_query_text}\n"""

                        contexttext6 = """ # COMMAND ----------\n"""

                        contexttext7 = f"""{variable_text}\n"""

                        # contexttext7b = f"""{query_text}\n"""

                        contexttext8 = f"""{print_df_results_text}\n"""

                        content_text = (
                            contexttext1
                            + contexttext2
                            + contexttext3
                            + contexttext4
                            + contexttext5
                        )
                        content_text = (
                            content_text + contexttext6 + contexttext7 + contexttext8
                        )
                        content_text = content_text.lstrip()

                        api_version = "/api/2.0"
                        api_command = "/workspace/import"
                        url = f"https://{databricks_instance_id}{api_version}{api_command}"

                        pipeline_name = pipeline_name.replace(data_product_id, "")
                        pipeline_name = pipeline_name.replace(".", "")
                        pipeline_name = data_product_id + "_" + pipeline_name

                        content_python = base64.b64encode(
                            content_text.encode("UTF-8")
                        ).decode("UTF-8")
                        new_path_python = str(
                            os.path.join(dir_name_python, pipeline_name)
                        )
                        sys.path.append(dir_name_python)
                        isdir = os.path.isdir(dir_name_python)
                        logger.info(f"dir_name_python: isdir:{isdir}")

                        ## Try to delete the file ##
                        try:
                            os.remove(new_path_python)
                        except (
                            OSError
                        ) as e:  ## if failed, report it back to the user ##
                            logger.info("Error: %s - %s." % (e.filename, e.strerror))

                        # Python
                        # post
                        data_python = {
                            "content": content_python,
                            "path": new_path_python,
                            "language": "PYTHON",
                            "overwrite": True,
                            "format": "SOURCE",
                        }
                        logger.info(f"------- Save Python {pipeline_name}  -------")
                        logger.info(f"url:{str(url)}")

                        headers_import = {
                            "Authorization": bearer,
                            "Accept": "application/json",
                        }
                        headers_redacted = str(headers_import).replace(
                            bearer, "[bearer REDACTED]"
                        )
                        logger.info(f"headers:{headers_redacted}")
                        logger.info(f"json:{str(data_python)}")

                        # response
                        response_python = requests.post(
                            url=url, json=data_python, headers=headers_import
                        )

                        try:
                            data = json.loads(response_python.text)
                            response_python_text = json.dumps(response_python.json())
                            logger.info("- response : success  -")
                            response_python_text_message = (
                                "Received SAVE-PYTHON-RESPONSE : "
                            )
                            response_python_text_message += (
                                f"{response_python.text} when posting to : {url}  "
                            )
                            response_python_text_message += f"to save python pipeline with sql query: {pipeline_name}"
                            response_python_text_message += f"to {new_path_python}"
                        except Exception as exception_check:
                            html_filter = HTMLFilter()
                            html_filter.feed(response_python.text)
                            response_python_text = html_filter.text
                            logger.info(f"- response : error - {str(exception_check)}")
                            logger.info(
                                f"Error SAVE-PYTHON-RESPONSE converting response text:{response_python_text} to json"
                            )

                        logger.info(response_python_text)
                        sys.path.append(dir_name_sql)

                        content_sql = base64.b64encode(
                            query_text_original.encode("UTF-8")
                        ).decode("UTF-8")
                        # replace period in file name
                        query_name = query_name.replace(".", "_")
                        new_path_sql = str(os.path.join(dir_name_sql, query_name))
                        ## Try to delete the file ##
                        try:
                            os.remove(new_path_sql)
                        except (
                            OSError
                        ) as e:  ## if failed, report it back to the user ##
                            logger.info("Error: %s - %s." % (e.filename, e.strerror))

                        data_sql = {
                            "content": content_sql,
                            "path": new_path_sql,
                            "language": "SQL",
                            "overwrite": True,
                            "format": "SOURCE",
                        }
                        logger.info("------- Save SQL ----------------")
                        logger.info(f"url:{str(url)}")
                        headers_redacted = str(headers).replace(
                            bearer, "[bearer REDACTED]"
                        )
                        logger.info(f"headers:{headers_redacted}")
                        logger.info(f"json:{str(data_sql)}")

                    # post
                response_sql = requests.post(url=url, json=data_sql, headers=headers)
                config_sql = {"response_sql": response_sql}
                return config_sql

            except Exception as ex:
                error_msg = "Error: %s", ex
                exc_info = sys.exc_info()
                logger_singleton.error_with_exception(error_msg, exc_info)
                raise


class HTMLFilter(HTMLParser):
    """
    A class that filters HTML content and extracts text data.

    Attributes:
        text (str): The extracted text from the HTML content.
    """

    text = ""

    def handle_data(self, data):
        self.text += data
