{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python-dotenv could not parse statement starting at line 1\n"
     ]
    }
   ],
   "source": [
    "## Set env vars\n",
    "from operator import itemgetter\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from dotenv import load_dotenv\n",
    "from langchain.globals import set_debug\n",
    "\n",
    "set_debug(True)\n",
    "\n",
    "from contextcrunch_langchain import ConversationCruncher\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = ChatOpenAI()\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Conversation Summary:\\n{history}\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory()\n",
    "memory.chat_memory.add_user_message(\"I have a ML api that i would like to monetize, so I want to create an API proxy that will handle users, authentication, signup, and billing. Can I use vercel for this?\")\n",
    "memory.chat_memory.add_ai_message(\"Yes, you can use Vercel to create an API proxy for your machine learning API that handles user management, authentication, signup, and billing. Vercel is primarily known for its capabilities in deploying and hosting web applications and static sites, but it can also be used for serverless functions, which you can leverage to build your API proxy. Here are some key points to consider when setting up your API proxy on Vercel: Serverless Functions: You can write serverless functions in Vercel to handle various aspects of your API, like user authentication, signup, and billing. These functions can be written in languages like JavaScript, TypeScript, Python, etc. Authentication: For handling user authentication, you can integrate third-party services like Auth0, Firebase Authentication, or build your own custom solution. These services can be integrated into your serverless functions. Database Integration: You'll likely need a database to store user information and other relevant data. Vercel can connect to various databases, and you can choose one based on your requirements. Billing and Subscription Management: For billing, you can use third-party services like Stripe, PayPal, or others. These services offer APIs that you can integrate into your Vercel application to handle subscriptions, one-time payments, and other billing-related activities. API Routing and Proxying: You can set up routes in your Vercel project to manage the requests to your machine learning API. This involves receiving requests, authenticating them, and then forwarding them to your actual ML API. Environment Variables: Vercel supports environment variables, which are essential for securely storing API keys, database connection strings, and other sensitive information. Scalability: Vercel offers good scalability options, which is beneficial if your API usage grows over time. Deployment and CI/CD: Vercel provides easy deployment options and integrates with various version control systems for continuous integration and deployment. When setting up your API proxy on Vercel, you'll need to write the code for the serverless functions, set up the necessary integrations with databases and authentication/billing services, and configure your project's deployment and routing settings. Always ensure to test your setup thoroughly to handle various user scenarios and security considerations.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: My favourite food is pizza\\nAI: Okay, I understand. Your favourite food is pizza.'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {'history': RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"), 'input': RunnablePassthrough()}\n",
    "    | ConversationCruncher()\n",
    "    | prompt\n",
    "    | model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is my initial goal?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<history,input>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is my initial goal?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<history,input> > 3:chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is my initial goal?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<history,input> > 4:chain:RunnablePassthrough] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is my initial goal?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<history,input> > 4:chain:RunnablePassthrough] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"What is my initial goal?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<history,input> > 3:chain:RunnableSequence > 4:chain:load_memory_variables] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is my initial goal?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<history,input> > 3:chain:RunnableSequence > 4:chain:load_memory_variables] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"history\": \"Human: I have a ML api that i would like to monetize, so I want to create an API proxy that will handle users, authentication, signup, and billing. Can I use vercel for this?\\nAI: Yes, you can use Vercel to create an API proxy for your machine learning API that handles user management, authentication, signup, and billing. Vercel is primarily known for its capabilities in deploying and hosting web applications and static sites, but it can also be used for serverless functions, which you can leverage to build your API proxy. Here are some key points to consider when setting up your API proxy on Vercel: Serverless Functions: You can write serverless functions in Vercel to handle various aspects of your API, like user authentication, signup, and billing. These functions can be written in languages like JavaScript, TypeScript, Python, etc. Authentication: For handling user authentication, you can integrate third-party services like Auth0, Firebase Authentication, or build your own custom solution. These services can be integrated into your serverless functions. Database Integration: You'll likely need a database to store user information and other relevant data. Vercel can connect to various databases, and you can choose one based on your requirements. Billing and Subscription Management: For billing, you can use third-party services like Stripe, PayPal, or others. These services offer APIs that you can integrate into your Vercel application to handle subscriptions, one-time payments, and other billing-related activities. API Routing and Proxying: You can set up routes in your Vercel project to manage the requests to your machine learning API. This involves receiving requests, authenticating them, and then forwarding them to your actual ML API. Environment Variables: Vercel supports environment variables, which are essential for securely storing API keys, database connection strings, and other sensitive information. Scalability: Vercel offers good scalability options, which is beneficial if your API usage grows over time. Deployment and CI/CD: Vercel provides easy deployment options and integrates with various version control systems for continuous integration and deployment. When setting up your API proxy on Vercel, you'll need to write the code for the serverless functions, set up the necessary integrations with databases and authentication/billing services, and configure your project's deployment and routing settings. Always ensure to test your setup thoroughly to handle various user scenarios and security considerations.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<history,input> > 3:chain:RunnableSequence > 5:chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"history\": \"Human: I have a ML api that i would like to monetize, so I want to create an API proxy that will handle users, authentication, signup, and billing. Can I use vercel for this?\\nAI: Yes, you can use Vercel to create an API proxy for your machine learning API that handles user management, authentication, signup, and billing. Vercel is primarily known for its capabilities in deploying and hosting web applications and static sites, but it can also be used for serverless functions, which you can leverage to build your API proxy. Here are some key points to consider when setting up your API proxy on Vercel: Serverless Functions: You can write serverless functions in Vercel to handle various aspects of your API, like user authentication, signup, and billing. These functions can be written in languages like JavaScript, TypeScript, Python, etc. Authentication: For handling user authentication, you can integrate third-party services like Auth0, Firebase Authentication, or build your own custom solution. These services can be integrated into your serverless functions. Database Integration: You'll likely need a database to store user information and other relevant data. Vercel can connect to various databases, and you can choose one based on your requirements. Billing and Subscription Management: For billing, you can use third-party services like Stripe, PayPal, or others. These services offer APIs that you can integrate into your Vercel application to handle subscriptions, one-time payments, and other billing-related activities. API Routing and Proxying: You can set up routes in your Vercel project to manage the requests to your machine learning API. This involves receiving requests, authenticating them, and then forwarding them to your actual ML API. Environment Variables: Vercel supports environment variables, which are essential for securely storing API keys, database connection strings, and other sensitive information. Scalability: Vercel offers good scalability options, which is beneficial if your API usage grows over time. Deployment and CI/CD: Vercel provides easy deployment options and integrates with various version control systems for continuous integration and deployment. When setting up your API proxy on Vercel, you'll need to write the code for the serverless functions, set up the necessary integrations with databases and authentication/billing services, and configure your project's deployment and routing settings. Always ensure to test your setup thoroughly to handle various user scenarios and security considerations.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<history,input> > 3:chain:RunnableSequence > 5:chain:RunnableLambda] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"Human: I have a ML api that i would like to monetize, so I want to create an API proxy that will handle users, authentication, signup, and billing. Can I use vercel for this?\\nAI: Yes, you can use Vercel to create an API proxy for your machine learning API that handles user management, authentication, signup, and billing. Vercel is primarily known for its capabilities in deploying and hosting web applications and static sites, but it can also be used for serverless functions, which you can leverage to build your API proxy. Here are some key points to consider when setting up your API proxy on Vercel: Serverless Functions: You can write serverless functions in Vercel to handle various aspects of your API, like user authentication, signup, and billing. These functions can be written in languages like JavaScript, TypeScript, Python, etc. Authentication: For handling user authentication, you can integrate third-party services like Auth0, Firebase Authentication, or build your own custom solution. These services can be integrated into your serverless functions. Database Integration: You'll likely need a database to store user information and other relevant data. Vercel can connect to various databases, and you can choose one based on your requirements. Billing and Subscription Management: For billing, you can use third-party services like Stripe, PayPal, or others. These services offer APIs that you can integrate into your Vercel application to handle subscriptions, one-time payments, and other billing-related activities. API Routing and Proxying: You can set up routes in your Vercel project to manage the requests to your machine learning API. This involves receiving requests, authenticating them, and then forwarding them to your actual ML API. Environment Variables: Vercel supports environment variables, which are essential for securely storing API keys, database connection strings, and other sensitive information. Scalability: Vercel offers good scalability options, which is beneficial if your API usage grows over time. Deployment and CI/CD: Vercel provides easy deployment options and integrates with various version control systems for continuous integration and deployment. When setting up your API proxy on Vercel, you'll need to write the code for the serverless functions, set up the necessary integrations with databases and authentication/billing services, and configure your project's deployment and routing settings. Always ensure to test your setup thoroughly to handle various user scenarios and security considerations.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<history,input> > 3:chain:RunnableSequence] [8ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"Human: I have a ML api that i would like to monetize, so I want to create an API proxy that will handle users, authentication, signup, and billing. Can I use vercel for this?\\nAI: Yes, you can use Vercel to create an API proxy for your machine learning API that handles user management, authentication, signup, and billing. Vercel is primarily known for its capabilities in deploying and hosting web applications and static sites, but it can also be used for serverless functions, which you can leverage to build your API proxy. Here are some key points to consider when setting up your API proxy on Vercel: Serverless Functions: You can write serverless functions in Vercel to handle various aspects of your API, like user authentication, signup, and billing. These functions can be written in languages like JavaScript, TypeScript, Python, etc. Authentication: For handling user authentication, you can integrate third-party services like Auth0, Firebase Authentication, or build your own custom solution. These services can be integrated into your serverless functions. Database Integration: You'll likely need a database to store user information and other relevant data. Vercel can connect to various databases, and you can choose one based on your requirements. Billing and Subscription Management: For billing, you can use third-party services like Stripe, PayPal, or others. These services offer APIs that you can integrate into your Vercel application to handle subscriptions, one-time payments, and other billing-related activities. API Routing and Proxying: You can set up routes in your Vercel project to manage the requests to your machine learning API. This involves receiving requests, authenticating them, and then forwarding them to your actual ML API. Environment Variables: Vercel supports environment variables, which are essential for securely storing API keys, database connection strings, and other sensitive information. Scalability: Vercel offers good scalability options, which is beneficial if your API usage grows over time. Deployment and CI/CD: Vercel provides easy deployment options and integrates with various version control systems for continuous integration and deployment. When setting up your API proxy on Vercel, you'll need to write the code for the serverless functions, set up the necessary integrations with databases and authentication/billing services, and configure your project's deployment and routing settings. Always ensure to test your setup thoroughly to handle various user scenarios and security considerations.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<history,input>] [20ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"history\": \"Human: I have a ML api that i would like to monetize, so I want to create an API proxy that will handle users, authentication, signup, and billing. Can I use vercel for this?\\nAI: Yes, you can use Vercel to create an API proxy for your machine learning API that handles user management, authentication, signup, and billing. Vercel is primarily known for its capabilities in deploying and hosting web applications and static sites, but it can also be used for serverless functions, which you can leverage to build your API proxy. Here are some key points to consider when setting up your API proxy on Vercel: Serverless Functions: You can write serverless functions in Vercel to handle various aspects of your API, like user authentication, signup, and billing. These functions can be written in languages like JavaScript, TypeScript, Python, etc. Authentication: For handling user authentication, you can integrate third-party services like Auth0, Firebase Authentication, or build your own custom solution. These services can be integrated into your serverless functions. Database Integration: You'll likely need a database to store user information and other relevant data. Vercel can connect to various databases, and you can choose one based on your requirements. Billing and Subscription Management: For billing, you can use third-party services like Stripe, PayPal, or others. These services offer APIs that you can integrate into your Vercel application to handle subscriptions, one-time payments, and other billing-related activities. API Routing and Proxying: You can set up routes in your Vercel project to manage the requests to your machine learning API. This involves receiving requests, authenticating them, and then forwarding them to your actual ML API. Environment Variables: Vercel supports environment variables, which are essential for securely storing API keys, database connection strings, and other sensitive information. Scalability: Vercel offers good scalability options, which is beneficial if your API usage grows over time. Deployment and CI/CD: Vercel provides easy deployment options and integrates with various version control systems for continuous integration and deployment. When setting up your API proxy on Vercel, you'll need to write the code for the serverless functions, set up the necessary integrations with databases and authentication/billing services, and configure your project's deployment and routing settings. Always ensure to test your setup thoroughly to handle various user scenarios and security considerations.\",\n",
      "  \"input\": \"What is my initial goal?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 6:chain:call] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"history\": \"Human: I have a ML api that i would like to monetize, so I want to create an API proxy that will handle users, authentication, signup, and billing. Can I use vercel for this?\\nAI: Yes, you can use Vercel to create an API proxy for your machine learning API that handles user management, authentication, signup, and billing. Vercel is primarily known for its capabilities in deploying and hosting web applications and static sites, but it can also be used for serverless functions, which you can leverage to build your API proxy. Here are some key points to consider when setting up your API proxy on Vercel: Serverless Functions: You can write serverless functions in Vercel to handle various aspects of your API, like user authentication, signup, and billing. These functions can be written in languages like JavaScript, TypeScript, Python, etc. Authentication: For handling user authentication, you can integrate third-party services like Auth0, Firebase Authentication, or build your own custom solution. These services can be integrated into your serverless functions. Database Integration: You'll likely need a database to store user information and other relevant data. Vercel can connect to various databases, and you can choose one based on your requirements. Billing and Subscription Management: For billing, you can use third-party services like Stripe, PayPal, or others. These services offer APIs that you can integrate into your Vercel application to handle subscriptions, one-time payments, and other billing-related activities. API Routing and Proxying: You can set up routes in your Vercel project to manage the requests to your machine learning API. This involves receiving requests, authenticating them, and then forwarding them to your actual ML API. Environment Variables: Vercel supports environment variables, which are essential for securely storing API keys, database connection strings, and other sensitive information. Scalability: Vercel offers good scalability options, which is beneficial if your API usage grows over time. Deployment and CI/CD: Vercel provides easy deployment options and integrates with various version control systems for continuous integration and deployment. When setting up your API proxy on Vercel, you'll need to write the code for the serverless functions, set up the necessary integrations with databases and authentication/billing services, and configure your project's deployment and routing settings. Always ensure to test your setup thoroughly to handle various user scenarios and security considerations.\",\n",
      "  \"input\": \"What is my initial goal?\"\n",
      "}\n",
      "\u001b[31;1m\u001b[1;3m[chain/error]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 6:chain:call] [30.02s] Chain run errored with error:\n",
      "\u001b[0m\"Exception('Error making request to https://contextcrunch.com/api/call: Expecting value: line 1 column 1 (char 0)')Traceback (most recent call last):\\n\\n\\n  File \\\"/Users/matt/Documents/GitHub/contextcrunch-langchain-python/env/lib/python3.11/site-packages/requests/models.py\\\", line 971, in json\\n    return complexjson.loads(self.text, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/usr/local/Cellar/python@3.11/3.11.4_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/__init__.py\\\", line 346, in loads\\n    return _default_decoder.decode(s)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/usr/local/Cellar/python@3.11/3.11.4_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/decoder.py\\\", line 337, in decode\\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/usr/local/Cellar/python@3.11/3.11.4_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/decoder.py\\\", line 355, in raw_decode\\n    raise JSONDecodeError(\\\"Expecting value\\\", s, err.value) from None\\n\\n\\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\\n\\n\\n\\nDuring handling of the above exception, another exception occurred:\\n\\n\\n\\nTraceback (most recent call last):\\n\\n\\n  File \\\"/Users/matt/Documents/GitHub/contextcrunch-langchain-python/env/lib/python3.11/site-packages/contextcrunch/client.py\\\", line 21, in _request\\n    return response.json()\\n           ^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/matt/Documents/GitHub/contextcrunch-langchain-python/env/lib/python3.11/site-packages/requests/models.py\\\", line 975, in json\\n    raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\\n\\n\\nrequests.exceptions.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\\n\\n\\n\\nDuring handling of the above exception, another exception occurred:\\n\\n\\n\\nTraceback (most recent call last):\\n\\n\\n  File \\\"/Users/matt/Documents/GitHub/contextcrunch-langchain-python/env/lib/python3.11/site-packages/langchain_core/runnables/base.py\\\", line 975, in _call_with_config\\n    context.run(\\n\\n\\n  File \\\"/Users/matt/Documents/GitHub/contextcrunch-langchain-python/env/lib/python3.11/site-packages/langchain_core/runnables/config.py\\\", line 323, in call_func_with_variable_args\\n    return func(input, **kwargs)  # type: ignore[call-arg]\\n           ^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/matt/Documents/GitHub/contextcrunch-langchain-python/env/lib/python3.11/site-packages/langchain_core/runnables/base.py\\\", line 2950, in _invoke\\n    output = call_func_with_variable_args(\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/matt/Documents/GitHub/contextcrunch-langchain-python/env/lib/python3.11/site-packages/langchain_core/runnables/config.py\\\", line 323, in call_func_with_variable_args\\n    return func(input, **kwargs)  # type: ignore[call-arg]\\n           ^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/matt/Documents/GitHub/contextcrunch-langchain-python/contextcrunch_langchain/context_cruncher.py\\\", line 67, in call\\n    compressed_context = self.client.compress(context, prompt, type='conversation')\\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/matt/Documents/GitHub/contextcrunch-langchain-python/env/lib/python3.11/site-packages/contextcrunch/client.py\\\", line 39, in compress\\n    result = self._request(\\\"call\\\", body)\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/matt/Documents/GitHub/contextcrunch-langchain-python/env/lib/python3.11/site-packages/contextcrunch/client.py\\\", line 23, in _request\\n    raise Exception(f\\\"Error making request to {request_url}: {e}\\\")\\n\\n\\nException: Error making request to https://contextcrunch.com/api/call: Expecting value: line 1 column 1 (char 0)\"\n",
      "\u001b[31;1m\u001b[1;3m[chain/error]\u001b[0m \u001b[1m[1:chain:RunnableSequence] [30.06s] Chain run errored with error:\n",
      "\u001b[0m\"Exception('Error making request to https://contextcrunch.com/api/call: Expecting value: line 1 column 1 (char 0)')Traceback (most recent call last):\\n\\n\\n  File \\\"/Users/matt/Documents/GitHub/contextcrunch-langchain-python/env/lib/python3.11/site-packages/requests/models.py\\\", line 971, in json\\n    return complexjson.loads(self.text, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/usr/local/Cellar/python@3.11/3.11.4_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/__init__.py\\\", line 346, in loads\\n    return _default_decoder.decode(s)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/usr/local/Cellar/python@3.11/3.11.4_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/decoder.py\\\", line 337, in decode\\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/usr/local/Cellar/python@3.11/3.11.4_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/decoder.py\\\", line 355, in raw_decode\\n    raise JSONDecodeError(\\\"Expecting value\\\", s, err.value) from None\\n\\n\\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\\n\\n\\n\\nDuring handling of the above exception, another exception occurred:\\n\\n\\n\\nTraceback (most recent call last):\\n\\n\\n  File \\\"/Users/matt/Documents/GitHub/contextcrunch-langchain-python/env/lib/python3.11/site-packages/contextcrunch/client.py\\\", line 21, in _request\\n    return response.json()\\n           ^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/matt/Documents/GitHub/contextcrunch-langchain-python/env/lib/python3.11/site-packages/requests/models.py\\\", line 975, in json\\n    raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\\n\\n\\nrequests.exceptions.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\\n\\n\\n\\nDuring handling of the above exception, another exception occurred:\\n\\n\\n\\nTraceback (most recent call last):\\n\\n\\n  File \\\"/Users/matt/Documents/GitHub/contextcrunch-langchain-python/env/lib/python3.11/site-packages/langchain_core/runnables/base.py\\\", line 1774, in invoke\\n    input = step.invoke(\\n            ^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/matt/Documents/GitHub/contextcrunch-langchain-python/env/lib/python3.11/site-packages/langchain_core/runnables/base.py\\\", line 3074, in invoke\\n    return self._call_with_config(\\n           ^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/matt/Documents/GitHub/contextcrunch-langchain-python/env/lib/python3.11/site-packages/langchain_core/runnables/base.py\\\", line 975, in _call_with_config\\n    context.run(\\n\\n\\n  File \\\"/Users/matt/Documents/GitHub/contextcrunch-langchain-python/env/lib/python3.11/site-packages/langchain_core/runnables/config.py\\\", line 323, in call_func_with_variable_args\\n    return func(input, **kwargs)  # type: ignore[call-arg]\\n           ^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/matt/Documents/GitHub/contextcrunch-langchain-python/env/lib/python3.11/site-packages/langchain_core/runnables/base.py\\\", line 2950, in _invoke\\n    output = call_func_with_variable_args(\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/matt/Documents/GitHub/contextcrunch-langchain-python/env/lib/python3.11/site-packages/langchain_core/runnables/config.py\\\", line 323, in call_func_with_variable_args\\n    return func(input, **kwargs)  # type: ignore[call-arg]\\n           ^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/matt/Documents/GitHub/contextcrunch-langchain-python/contextcrunch_langchain/context_cruncher.py\\\", line 67, in call\\n    compressed_context = self.client.compress(context, prompt, type='conversation')\\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/matt/Documents/GitHub/contextcrunch-langchain-python/env/lib/python3.11/site-packages/contextcrunch/client.py\\\", line 39, in compress\\n    result = self._request(\\\"call\\\", body)\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/matt/Documents/GitHub/contextcrunch-langchain-python/env/lib/python3.11/site-packages/contextcrunch/client.py\\\", line 23, in _request\\n    raise Exception(f\\\"Error making request to {request_url}: {e}\\\")\\n\\n\\nException: Error making request to https://contextcrunch.com/api/call: Expecting value: line 1 column 1 (char 0)\"\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Error making request to https://contextcrunch.com/api/call: Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/GitHub/contextcrunch-langchain-python/env/lib/python3.11/site-packages/requests/models.py:971\u001b[0m, in \u001b[0;36mResponse.json\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    970\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 971\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcomplexjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    973\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.11/3.11.4_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.11/3.11.4_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.11/3.11.4_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/GitHub/contextcrunch-langchain-python/env/lib/python3.11/site-packages/contextcrunch/client.py:21\u001b[0m, in \u001b[0;36mContextCrunchClient._request\u001b[0;34m(self, endpoint, data)\u001b[0m\n\u001b[1;32m     20\u001b[0m     response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(request_url, headers\u001b[38;5;241m=\u001b[39mheader, json\u001b[38;5;241m=\u001b[39mdata)\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Documents/GitHub/contextcrunch-langchain-python/env/lib/python3.11/site-packages/requests/models.py:975\u001b[0m, in \u001b[0;36mResponse.json\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    973\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n\u001b[0;32m--> 975\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RequestsJSONDecodeError(e\u001b[38;5;241m.\u001b[39mmsg, e\u001b[38;5;241m.\u001b[39mdoc, e\u001b[38;5;241m.\u001b[39mpos)\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_openai_callback() \u001b[38;5;28;01mas\u001b[39;00m cb:\n\u001b[0;32m----> 2\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is my initial goal?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcb\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/GitHub/contextcrunch-langchain-python/env/lib/python3.11/site-packages/langchain_core/runnables/base.py:1774\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 1774\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1775\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1776\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   1777\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1778\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1779\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1780\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Documents/GitHub/contextcrunch-langchain-python/env/lib/python3.11/site-packages/langchain_core/runnables/base.py:3074\u001b[0m, in \u001b[0;36mRunnableLambda.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3072\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Invoke this runnable synchronously.\"\"\"\u001b[39;00m\n\u001b[1;32m   3073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunc\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 3074\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3075\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3076\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3077\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3078\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3079\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3080\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3081\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   3082\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot invoke a coroutine function synchronously.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3083\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse `ainvoke` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3084\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/GitHub/contextcrunch-langchain-python/env/lib/python3.11/site-packages/langchain_core/runnables/base.py:975\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m    971\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[1;32m    972\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(var_child_runnable_config\u001b[38;5;241m.\u001b[39mset, child_config)\n\u001b[1;32m    973\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m    974\u001b[0m         Output,\n\u001b[0;32m--> 975\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    978\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    979\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    983\u001b[0m     )\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    985\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/Documents/GitHub/contextcrunch-langchain-python/env/lib/python3.11/site-packages/langchain_core/runnables/config.py:323\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    322\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 323\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/contextcrunch-langchain-python/env/lib/python3.11/site-packages/langchain_core/runnables/base.py:2950\u001b[0m, in \u001b[0;36mRunnableLambda._invoke\u001b[0;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m   2948\u001b[0m                 output \u001b[38;5;241m=\u001b[39m chunk\n\u001b[1;32m   2949\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2950\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2951\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   2952\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2953\u001b[0m \u001b[38;5;66;03m# If the output is a runnable, invoke it\u001b[39;00m\n\u001b[1;32m   2954\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, Runnable):\n",
      "File \u001b[0;32m~/Documents/GitHub/contextcrunch-langchain-python/env/lib/python3.11/site-packages/langchain_core/runnables/config.py:323\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    322\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 323\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/contextcrunch-langchain-python/contextcrunch_langchain/context_cruncher.py:67\u001b[0m, in \u001b[0;36mConversationCruncher.call\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhistory\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     66\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 67\u001b[0m compressed_context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompress\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconversation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconhistorytext\u001b[39m\u001b[38;5;124m\"\u001b[39m: compressed_context, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt}\n",
      "File \u001b[0;32m~/Documents/GitHub/contextcrunch-langchain-python/env/lib/python3.11/site-packages/contextcrunch/client.py:39\u001b[0m, in \u001b[0;36mContextCrunchClient.compress\u001b[0;34m(self, context, prompt, compression_ratio, type, concat_prompt)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mType must be either \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrag\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconversation\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m body \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m: context,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     }\n\u001b[1;32m     38\u001b[0m }\n\u001b[0;32m---> 39\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcall\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m concat_prompt:\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/Documents/GitHub/contextcrunch-langchain-python/env/lib/python3.11/site-packages/contextcrunch/client.py:23\u001b[0m, in \u001b[0;36mContextCrunchClient._request\u001b[0;34m(self, endpoint, data)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError making request to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequest_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: Error making request to https://contextcrunch.com/api/call: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "with get_openai_callback() as cb:\n",
    "    result = chain.invoke(\"What is my initial goal?\")\n",
    "    print(f'{result}')\n",
    "    print(f'tokens: {cb}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\")\n",
    "    )\n",
    "    | prompt\n",
    "    | model\n",
    ")\n",
    "small_chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\")\n",
    "    )\n",
    "    | prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello Bob! How can I assist you today?')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {\"input\": \"hi im bob\"}\n",
    "response = chain.invoke(inputs)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context(inputs, {\"output\": response.content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='hi im bob'),\n",
       "  AIMessage(content='Hello Bob! How can I assist you today?')]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: Tokens Used: 47\n",
      "\tPrompt Tokens: 42\n",
      "\tCompletion Tokens: 5\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $7.3e-05\n"
     ]
    }
   ],
   "source": [
    "with get_openai_callback() as cb:\n",
    "    inputs = {\"input\": \"whats my name\"}\n",
    "    response = chain.invoke(inputs)\n",
    "    response\n",
    "    print(f'tokens: {cb}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful chatbot'), HumanMessage(content='hi im bob'), AIMessage(content='Hello Bob! How can I assist you today?'), HumanMessage(content='whats my name')])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {\"input\": \"whats my name\"}\n",
    "small_chain.invoke(inputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
