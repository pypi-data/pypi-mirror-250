#!/usr/bin/env python
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, GradientBoostingRegressor
from sklearn.linear_model import Lasso, LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import make_scorer, mean_squared_error
from sklearn.ensemble import RandomForestRegressor

from datetime import datetime
import os

num_procs = 20
model_names = ["Random Forest"]
condition_model_names = ["Random Forest", "Gradient Boosting", "Logistic Regression"]
condition_model_objs = [GridSearchCV(RandomForestClassifier(n_estimators=500, class_weight="auto", n_jobs=num_procs,
                                                            min_samples_leaf=1, verbose=1),
                                     param_grid=[dict(max_features=["sqrt", 30, 50],
                                                      )],
                                     scoring="roc_auc",
                                     n_jobs=1),
                        GradientBoostingClassifier(n_estimators=200, learning_rate=0.05, max_depth=5),
                        LogisticRegression(penalty='l1')
                        ]
model_objs = [RandomForestClassifier(n_estimators=500, max_features=30, max_depth=6, n_jobs=num_procs)]
dist_model_names = ["Random Forest", "Gradient Boosting", "Lasso"]
#dist_model_names = ["Random Forest CV"]
mse_scorer = make_scorer(mean_squared_error, greater_is_better=False)
dist_model_objs = [GridSearchCV(RandomForestRegressor(n_estimators=500, n_jobs=num_procs, min_samples_leaf=1),
                                param_grid=[dict(max_features=["sqrt", 30, 50],
                                                 )],
                                scoring=mse_scorer,
                                n_jobs=1
                                ),
                   GradientBoostingRegressor(n_estimators=500, max_depth=5, learning_rate=0.05, verbose=2),
                   Lasso()
                   ]
storm_variables=["uh_max", "wupmax", "cqgmax", "pwat",
                ]
potential_variables=["mlcape", "mlcins", "mucape", "mucins", "sblcl", "shr06", "tmp500",
                     "tmp700", "sph850", "sph700", "sph500",
                     "u700", "v700"]
tendency_variables=[]
shape_variables=["area", "eccentricity", "major_axis_length", "minor_axis_length", "orientation"]
variable_statistics=["mean", "max", "min", "std"]
input_columns = []
for var in storm_variables:
    for stat in variable_statistics:
        input_columns.append(var + "_" + stat)
for var in potential_variables:
    for stat in variable_statistics:
        input_columns.append(var + "-potential_" + stat)
input_columns += shape_variables
ensemble_members = ["wrf-core01_arw"] + ["wrf-core{0:02d}_arw".format(m) for m in range(3, 11)]
ensemble_members += ["wrf-s_phys_rad{0:02d}_arw".format(m) for m in range(2, 11)]
scratch_path = "/hail/djgagne/"
fore_date = datetime.strptime(datetime.utcnow().strftime("%Y%m%d"), "%Y%m%d")
config = dict(ensemble_name="SSEF",
              ensemble_members=ensemble_members,
              num_procs=num_procs,
              start_dates={"train": datetime(2016, 4, 20), "forecast": fore_date},
              end_dates={"train": datetime(2016, 5, 27), "forecast": fore_date},
              start_hour=13,
              end_hour=36,
              train_data_path=scratch_path + "track_data_spring2016_cqg_patch_csv/",
              forecast_data_path=scratch_path + "track_data_spring2017_cqg_patch_csv/",
              member_files={"train": scratch_path + "member_info_ssef_spring2016.csv",
                            "forecast": scratch_path + "member_info_ssef_spring2017.csv"},
              data_format="csv",
              group_col="Microphysics",
              condition_model_names=condition_model_names,
              condition_model_objs=condition_model_objs,
              condition_input_columns=input_columns,
              condition_output_column="Matched",
              condition_threshold=0.5,
              size_distribution_model_names=dist_model_names,
              size_distribution_model_objs=dist_model_objs,
              size_distribution_input_columns=input_columns,
              size_distribution_output_columns=["Shape", "Scale"],
              size_distribution_loc=13,
              load_models=True,
              model_path=scratch_path + "track_models_cqg_patch_spring2016/",
              metadata_columns=["Track_ID", "Step_ID", "Ensemble_Member", "Forecast_Hour"],
              data_json_path=scratch_path + "track_data_spring2016_cqg_closest_json/",
              forecast_json_path=scratch_path + "track_forecasts_spring2017_cqg_patch_json/",
              forecast_csv_path=scratch_path + "track_forecasts_spring2017_cqg_patch_csv/",
              netcdf_path=scratch_path + "track_data_spring2017_cqg_patch_nc/",
              ensemble_variables=["uh_max", "hailsz", "cqgmax"],
              ensemble_variable_thresholds={"dist": [25, 50],
                                            "uh_max": [75, 150],
                                            "hailsz": [25, 50],
                                            "cqgmax": [20, 40]},
              ml_grid_method="gamma",
              neighbor_condition_model="Random Forest",
              neighbor_radius=[14, 28],
              neighbor_sigma=[5, 20],
              ensemble_consensus_path=scratch_path + "hail_consensus_ssef_cqg_closest_2016/",
              ensemble_data_path="/sharp/djgagne/spring2015_nc/",
              model_map_file="/home/djgagne/hagelslag/mapfiles/ssef2017.map",
              ml_grid_percentiles=["mean", 90],
              grib_path=scratch_path + "hail_forecasts_grib2_ssef_cqg_patch_2017/",
              single_step=True,
              )
