#!/usr/bin/env python
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import MultiTaskElasticNetCV
from sklearn.grid_search import GridSearchCV
from sklearn.metrics import make_scorer, mean_squared_error
from datetime import datetime


num_procs = 25
model_names = ["Random Forest"]
condition_model_objs = [GridSearchCV(RandomForestClassifier(n_estimators=500, 
                                                            class_weight="auto", 
                                                            max_depth=6,
                                                            n_jobs=num_procs),
                        param_grid=[dict(max_features=["sqrt", 20, 30, 50])],
                        scoring="roc_auc",
                        n_jobs=1)]
# Models for predicting a gamma size pdf
dist_model_names = ["Random Forest", "Elastic Net", "Random Forest CV"]
mse_scorer = make_scorer(mean_squared_error, greater_is_better=False)
model_objs = [RandomForestClassifier(n_estimators=500, max_features=30, max_depth=6, n_jobs=num_procs)] 
dist_model_objs = [RandomForestRegressor(n_estimators=500,
                                         max_depth=6,
                                         max_features="sqrt",
                                         n_jobs=num_procs),
                   MultiTaskElasticNetCV(l1_ratio=[0.1, 0.5, 0.7, 0.9, 0.95, 0.99], n_jobs=num_procs),
                   GridSearchCV(RandomForestRegressor(n_estimators=500, max_depth=6, n_jobs=num_procs),
                                param_grid=[dict(max_features=["sqrt", 20, 30, 50])],
                                scoring=mse_scorer,
                                n_jobs=1
                                ),
                   ]

variables = ["UP_HELI_MAX", "GRPL_MAX", "W_UP_MAX", "W_DN_MAX", "WSPD10MAX", "UP_HELI_MIN"]
var_stats = ["mean", "max", "min", "std", "mean_dt", "max_dt"]
# variables describing the shape of the object have been added
shape_variables = ["area", "eccentricity", "major_axis_length", "minor_axis_length", "orientation",
                   "extent"] + ["weighted_moments_hu_{0:d}".format(h) for h in range(7)]
input_columns = ["Forecast_Hour", "Valid_Hour_UTC", "Duration_Step", "Duration_Total", "Centroid_Lon", "Centroid_Lat"]
for var in variables:
    for stat in var_stats:
        input_columns.append(var + "_" + stat)
input_columns.extend(shape_variables)
# change the scratch path to your directory
scratch_path = "/sharp/djgagne/"
ensemble_members = ["mem{0:d}".format(m) for m in range(1, 11)]
config = dict(ensemble_name="NCAR", # name of the ensemble; should match your data files
              ensemble_members=ensemble_members, # ensemble member list
              num_procs=num_procs, # number of processors; used in neighborhood probability generation
              start_dates={"train": datetime(2015, 5, 15), "forecast": datetime(2015, 5, 15)}, # beginning run dates
              end_dates={"train": datetime(2015, 5, 31), "forecast": datetime(2015, 5, 31)}, # ending run dates
              start_hour=12, # first forecast hour
              end_hour=36, # last forecast hour
              map_filename="/home/djgagne/hagelslag/mapfiles/ncar_ensemble_map_2015.txt",
              train_data_path=scratch_path + "track_data_ncar_2015_csv/",
              forecast_data_path=scratch_path + "track_data_ncar_2015_csv/",
              member_files={"train": scratch_path + "member_info_ncar_2015.csv",
                            "forecast": scratch_path + "member_info_ncar_2015.csv"},
              data_format="csv",
              group_col="Microphysics",
              condition_model_names=model_names,
              condition_model_objs=condition_model_objs,
              condition_input_columns=input_columns,
              condition_output_column="Hail_Size",
              output_threshold=5,
              size_model_names=model_names,
              size_model_objs=model_objs,
              size_input_columns=input_columns,
              size_output_column="Hail_Size",
              size_range_params=(5, 100, 5),
              # Size distribution models predict the scale parameter of the gamma distribution
              # Then a linear regression is fitted between the log of the predicted scale parameter
              # and the log of the observed shape parameter.
              size_distribution_model_names=dist_model_names,
              size_distribution_model_objs=dist_model_objs,
              size_distribution_input_columns=input_columns,
              size_distribution_output_columns=["Shape", "Location", "Scale"],
              track_model_names=model_names,
              track_model_objs=model_objs,
              track_input_columns=input_columns,
              track_output_columns={"translation-x": "Translation_Error_X",
                                    "translation-y": "Translation_Error_Y",
                                    "start-time": "Start_Time_Error"},
              track_output_ranges={"translation-x": (-240000, 240000, 24000),
                                   "translation-y": (-240000, 240000, 24000),
                                   "start-time": (-6, 6, 1),
                                   },
              load_models=True,
              model_path=scratch_path + "track_models_ncar_2015/20150515/",
              metadata_columns=["Track_ID", "Step_ID"],
              data_json_path=scratch_path + "track_data_ncar_2015_json/",
              forecast_json_path=scratch_path + "track_forecasts_ncar_2015_json/",
              copula_file=scratch_path + "track_copulas_ncar_2015.pkl",
              num_track_samples=1000,
              sampler_thresholds=[25, 50],
              sampler_out_path=scratch_path + "track_samples_ncar_2015/",
              # Variables used for generating neighborhood probabilities
              ensemble_variables=["HAIL_MAX2D", "HAIL_MAXK1", "UP_HELI_MAX"],
              # Neighborhood probability thresholds
              ensemble_variable_thresholds={"UP_HELI_MAX": [75, 150],
                                            "HAIL_MAXK1":[25, 50],
                                            "HAIL_MAX2D": [25, 50]},
              # Setting ml_grid_method as gamma will use the new models to generate the neighborhood probabilities
              # To use the bin models, set ml_grid_method="mean"
              ml_grid_method="gamma",
              neighbor_radius=[14, 28],
              neighbor_sigma=[5, 20],
              # Dimensions of the NCAR ensemble grid (y, x)
              grid_shape=(985, 1580),
              # Output path of neighborhood probability netCDF files.
              ensemble_consensus_path=scratch_path + "ensemble_consensus_ncar_2015/",
              # Path to ensemble data files
              ensemble_data_path="/glade/scratch/sobash/RT2015/",
              # If true, each model time step is stored in a separate netCDF file. False if they are grouped together.
              single_step=False,
              )
