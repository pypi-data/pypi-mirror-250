#!/usr/bin/env python
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import MultiTaskElasticNetCV, LogisticRegression
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
from sklearn.metrics import make_scorer, mean_squared_error
from datetime import datetime
import os
import numpy as np

num_procs = 16
condition_model_names = ["Decision Tree Small", "Decision Tree Fully Grown",
                         "Random Forest Small All Features",
                         "Random Forest Small Feature Subset",
                         "Random Forest Fully Grown All Features",
                         "Random Forest Fully Grown Feature Subset", "Logistic Regression"]
condition_model_objs = [DecisionTreeClassifier(max_depth=4),
                        DecisionTreeClassifier(min_samples_leaf=1),
                        RandomForestClassifier(n_estimators=1000, max_depth=4, max_features=None,
                                               n_jobs=num_procs),
                        RandomForestClassifier(n_estimators=1000, max_depth=4, max_features="sqrt",
                                               n_jobs=num_procs),
                        RandomForestClassifier(n_estimators=1000, min_samples_leaf=1, max_features=None,
                                               n_jobs=num_procs),
                        RandomForestClassifier(n_estimators=1000, min_samples_leaf=1, max_features="sqrt",
                                               n_jobs=num_procs),
                        LogisticRegression()]
# Models for predicting a gamma size pdf
mse_scorer = make_scorer(mean_squared_error, greater_is_better=False)
model_names = ["Random Forest"]
model_objs = [RandomForestClassifier(n_estimators=10, max_features=30, min_samples_leaf=1, n_jobs=1)]
dist_model_names = ["Decision Tree Small", "Decision Tree Fully Grown", "Random Forest Small All Features",
                    "Random Forest Small Feature Subset", "Random Forest Fully Grown All Features",
                    "Random Forest Fully Grown Feature Subset", "Elastic Net"]
dist_model_objs = [DecisionTreeRegressor(max_depth=4), DecisionTreeRegressor(min_samples_leaf=1),
                   RandomForestRegressor(n_estimators=1000, max_depth=4, max_features=None,
                                          n_jobs=num_procs),
                   RandomForestRegressor(n_estimators=1000, max_depth=4, max_features="sqrt",
                                          n_jobs=num_procs),
                   RandomForestRegressor(n_estimators=1000, min_samples_leaf=1, max_features=None,
                                         n_jobs=num_procs),
                   RandomForestRegressor(n_estimators=1000, min_samples_leaf=1, max_features="sqrt", n_jobs=num_procs),
                   MultiTaskElasticNetCV(alphas=[0.5], n_alphas=1, l1_ratio=np.array([0.1, 0.5, 0.9, 0.99]),
                                         n_jobs=num_procs,
                                         cv=5)]

storm_variables = ["UP_HELI_MAX", "GRPL_MAX", "W_UP_MAX", "W_DN_MAX",  "REFD_MAX"]
potential_variables = ["UBSHR6", "VBSHR6", "PWAT", "SRH3", "LCL_HEIGHT", "CAPE_SFC",
                       "CIN_SFC"]
tendency_variables=[]
shape_variables = []
variable_statistics=["percentile_50"]
# variables describing the shape of the object have been added
input_columns = ["Valid_Hour_UTC", "Duration_Step"]
for var in storm_variables:
    for stat in variable_statistics:
        input_columns.append(var + "_" + stat)
for var in potential_variables:
    for stat in variable_statistics:
        input_columns.append(var + "-potential_" + stat)
for var in tendency_variables:
    for stat in variable_statistics:
        input_columns.append(var + "-tendency_" + stat)
input_columns.extend(shape_variables)
print input_columns
# change the scratch path to your directory
scratch_path = "/glade/p/work/dgagne/"
ensemble_members = ["mem{0:d}".format(m) for m in range(1, 11)]
#train_start = [datetime(2015,5,1), datetime(2015, 5, 15), datetime(2015,6,1), datetime(2015,6,15), datetime(2015,7,1)]
#train_end = [datetime(2015,5,14), datetime(2015, 5, 31), datetime(2015,6,14), datetime(2015,6,30), datetime(2015,7,14)]
#forecast_start = [datetime(2015, 5, 15), datetime(2015,6,1), datetime(2015,6,15), datetime(2015,7,1), datetime(2015,7,15)]
#forecast_end = [datetime(2015, 5, 31), datetime(2015,6,14), datetime(2015,6,30), datetime(2015,7,14), datetime(2015,7,30)]
train_start = datetime(2015, 5, 1)
train_end = datetime(2015, 6, 15)
forecast_start = datetime(2015, 6, 16)
forecast_end = datetime(2015, 7, 30)
#time_index = int(os.environ["time_index"])
config = dict(ensemble_name="NCAR", # name of the ensemble; should match your data files
              ensemble_members=ensemble_members, # ensemble member list
              num_procs=num_procs, # number of processors; used in neighborhood probability generation
              start_dates={"train": train_start, "forecast": forecast_start}, # beginning run dates
              end_dates={"train": train_end, "forecast": forecast_end}, # ending run dates
              start_hour=13, # first forecast hour
              end_hour=36, # last forecast hour
              map_filename="/glade/u/home/dgagne/hagelslag/mapfiles/ncar_ensemble_map_2015.txt",
              train_data_path=scratch_path + "track_data_ncar_2015_csv/",
              forecast_data_path=scratch_path + "track_data_ncar_2015_csv/",
              member_files={"train": scratch_path + "member_info_ncar_2015.csv",
                            "forecast": scratch_path + "member_info_ncar_2015.csv"},
              data_format="csv",
              group_col="Microphysics",
              condition_model_names=condition_model_names,
              condition_model_objs=condition_model_objs,
              condition_input_columns=input_columns,
              condition_output_column="Hail_Size",
              condition_threshold=0.5,
              output_threshold=5,
              size_model_names=model_names,
              size_model_objs=model_objs,
              size_input_columns=input_columns,
              size_output_column="Hail_Size",
              size_range_params=(5, 100, 5),
              # Size distribution models predict the scale parameter of the gamma distribution
              # Then a linear regression is fitted between the log of the predicted scale parameter
              # and the log of the observed shape parameter.
              size_distribution_model_names=dist_model_names,
              size_distribution_model_objs=dist_model_objs,
              size_distribution_input_columns=input_columns,
              size_distribution_output_columns=["Shape", "Scale"],
              size_distribution_calibrate=False,
              track_model_names=model_names,
              track_model_objs=model_objs,
              track_input_columns=input_columns,
              track_output_columns={"translation-x": "Translation_Error_X",
                                    "translation-y": "Translation_Error_Y",
                                    "start-time": "Start_Time_Error"},
              track_output_ranges={"translation-x": (-240000, 240000, 24000),
                                   "translation-y": (-240000, 240000, 24000),
                                   "start-time": (-6, 6, 1),
                                   },
              load_models=True,
              model_path=scratch_path + "track_models_ncar_2015_interp/",
              metadata_columns=["Track_ID", "Step_ID"],
              data_json_path=scratch_path + "track_data_ncar_2015_json/",
              forecast_json_path=scratch_path + "track_forecasts_ncar_2015_interp_json/",
              copula_file=scratch_path + "track_copulas_ncar_2015.pkl",
              num_track_samples=1000,
              sampler_thresholds=[25, 50],
              sampler_out_path=scratch_path + "track_samples_ncar_2015/", 
              neighbor_condition_model="Random Forest Fully Grown Feature Subset",
              # Variables used for generating neighborhood probabilities
              ensemble_variables=["HAIL_MAX2D", "HAIL_MAXK1", "UP_HELI_MAX", "GRPL_MAX"],
              # Neighborhood probability thresholds
              ensemble_variable_thresholds={"UP_HELI_MAX": [75, 150],
                                            "HAIL_MAXK1":[25, 50],
                                            "HAIL_MAX2D": [25, 50],
                                            "GRPL_MAX": [20, 40]},
              # Setting ml_grid_method as gamma will use the new models to generate the neighborhood probabilities
              # To use the bin models, set ml_grid_method="mean"
              ml_grid_method="gamma",
              neighbor_radius=[14, 28],
              neighbor_sigma=[5, 20],
              # Dimensions of the NCAR ensemble grid (y, x)
              grid_shape=(985, 1580),
              # Output path of neighborhood probability netCDF files.
              ensemble_consensus_path="/glade/scratch/dgagne/ensemble_consensus_ncar_2015/",
              # Path to ensemble data files
              ensemble_data_path="/glade/scratch/sobash/RT2015/",
              model_map_file="/glade/u/home/dgagne/hagelslag/mapfiles/ncar_ensemble_map_2015.txt",
              ml_grid_percentiles=[10, "mean", 90],
              grib_path=scratch_path + "hail_forecasts_grib2_ncar_2015/",
              # If true, each model time step is stored in a separate netCDF file. False if they are grouped together.
              single_step=False,
              )
if not os.access(config["model_path"], os.R_OK):
    os.mkdir(config["model_path"])
